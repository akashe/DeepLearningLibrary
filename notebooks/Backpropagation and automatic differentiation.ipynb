{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop and automatic differentiation:\n",
    "\n",
    "[My purpose: To dump all that I learnt. To make/express it the best way I can]\n",
    "Few months ago I was trying to understand how learning actually happens and it struck me that I understood backprop in a very shallow way. So, I decided to understand it in depth and see how it is actually implemented in big neural networks. It will be a long post. My purpose is dual. First to keep this as a self reference. second, the information on all the subtopics is not available at one place, so I thought of compiling it here so it may  help people looking to dive deep into backprop.\n",
    "\n",
    "Intro to backprop:\n",
    "    List of good references to understand basics of bakprop\n",
    "    chain rule primer\n",
    "Modular structure of backprop:\n",
    "    the three functions\n",
    "Calculating gradients using jacobian products:\n",
    "    introduce vjp function\n",
    "Calculating gradients using automatic differentitation\n",
    "    how it converts into sequntial \n",
    "    autodiff lib\n",
    "Compuational graphs\n",
    "    static vs dynamic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
