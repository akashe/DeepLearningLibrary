{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Checking Vector Jacobian Products For a simple neural net\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later increase dimensions to include batches\n",
    "\n",
    "#FORWARD PASS\n",
    "\n",
    "x = torch.randn(3,1,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(4,3,dtype= torch.float)\n",
    "b1 = torch.randn(4,1,dtype= torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1403],\n",
      "        [ 0.6532],\n",
      "        [-0.1662],\n",
      "        [-0.4212]])\n"
     ]
    }
   ],
   "source": [
    "print(W1.mm(x)+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = W1.mm(x)+b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = F.relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.2695, -0.7558])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.randn(2,4,dtype= torch.float)\n",
    "b2 = torch.randn(2,1,dtype= torch.float)\n",
    "y = W2.mm(h) + b2\n",
    "y.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akashe/anaconda3/envs/latest_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "L = F.mse_loss(t,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3472)\n"
     ]
    }
   ],
   "source": [
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_bar = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2695],\n",
      "        [-1.7558]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akashe/anaconda3/envs/latest_pytorch/lib/python3.6/site-packages/torch/autograd/functional.py:251: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  outputs = func(*inputs)\n"
     ]
    }
   ],
   "source": [
    "y_bar = vjp(func=F.mse_loss,inputs=(y,t))[1][0]\n",
    "print(y_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2.3472), (tensor([[ 1.2695],\n",
      "        [-1.7558]]), tensor([0.2431, 0.2431])))\n"
     ]
    }
   ],
   "source": [
    "print(vjp(func=F.mse_loss,inputs=(y,t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nActually here derivative of t is wrong. It shud just be negative of y derivatives. It comes out right in case \\nof batch products\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Actually here derivative of t is wrong. It shud just be negative of y derivatives. It comes out right in case \n",
    "of batch products\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2695],\n",
      "        [-0.7558]])\n",
      "(tensor([[ 0.1781,  0.8292,  0.0000,  0.0000],\n",
      "        [-0.2463, -1.1468, -0.0000, -0.0000]]), tensor([[-2.6816],\n",
      "        [ 1.0106],\n",
      "        [ 2.1407],\n",
      "        [ 1.6628]]), tensor([[ 1.2695],\n",
      "        [-1.7558]]))\n"
     ]
    }
   ],
   "source": [
    "def affine(W,h,b):\n",
    "    return W.mm(h)+ b\n",
    "print(vjp(func=affine,inputs=(W2,h,b2),v = y_bar)[0])\n",
    "print(vjp(func=affine,inputs=(W2,h,b2),v = y_bar)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1403],\n",
      "        [0.6532],\n",
      "        [0.0000],\n",
      "        [0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.1403],\n",
      "        [0.6532],\n",
      "        [0.0000],\n",
      "        [0.0000]]), tensor([[-2.6816],\n",
      "        [ 1.0106],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]]))\n"
     ]
    }
   ],
   "source": [
    "h_bar = vjp(func=affine,inputs=(W2,h,b2),v = y_bar)[1][1]\n",
    "z_bar = vjp(func= F.relu, inputs= z,v = h_bar)\n",
    "print(z_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDimensions and values of backward gradients correct. Only problem is ouput of func is calculated twice once in\\nforward pass and second in vjp. I will also have to take care of not updating variables untill both forward and \\nbackward passes are done.\\n\\nNow trying a smaller network with batch size\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Dimensions and values of backward gradients correct. Only problem is ouput of func is calculated twice once in\n",
    "forward pass and second in vjp. I will also have to take care of not updating variables untill both forward and \n",
    "backward passes are done.\n",
    "\n",
    "Now trying a smaller network with batch size\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "x_ = torch.randn([batch_size,3,10],dtype=torch.float)\n",
    "W_ = torch.randn([10,3],dtype= torch.float)\n",
    "b_ = torch.randn([10,3,3], dtype = torch.float)\n",
    "\n",
    "#torch.mm() works only for 2D matrices\n",
    "#torch.matmul does batchwise multiplication if second tensor's dim > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1403],\n",
      "        [ 0.6532],\n",
      "        [-0.1662],\n",
      "        [-0.4212]])\n"
     ]
    }
   ],
   "source": [
    "z_ = x_.matmul(W_) + b_ \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 5.7178,  1.0459,  0.0000],\n",
      "         [ 3.9104,  0.0000,  0.0000],\n",
      "         [ 0.3138,  0.0000,  1.6776]],\n",
      "\n",
      "        [[ 0.0000,  2.8858,  3.5021],\n",
      "         [ 0.9874,  3.1130,  1.7054],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  1.3910,  0.6841],\n",
      "         [ 1.9745,  2.1534,  0.0000],\n",
      "         [ 1.0571,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 5.3951,  2.8337,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.7631],\n",
      "         [ 0.1244,  4.7911,  0.0000]],\n",
      "\n",
      "        [[ 2.0741,  2.2804,  0.0000],\n",
      "         [11.2952,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.7330,  1.0997]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5447,  0.0000,  5.3743],\n",
      "         [ 8.8495,  5.8250,  0.0000]],\n",
      "\n",
      "        [[ 1.7818,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.6814,  0.0000],\n",
      "         [ 0.0000,  3.8262,  0.0000]],\n",
      "\n",
      "        [[ 1.7551,  0.0000,  2.6309],\n",
      "         [ 0.0000,  1.7675,  0.0000],\n",
      "         [ 1.8795,  0.0000,  3.4326]],\n",
      "\n",
      "        [[ 4.5406,  0.0365,  0.9930],\n",
      "         [ 0.0000,  0.0000,  0.0000],\n",
      "         [ 0.1580,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  2.9571,  0.0000],\n",
      "         [ 0.7334,  0.0000,  0.0000],\n",
      "         [ 4.6988,  1.9951,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "y_ = F.relu(z_)\n",
    "print(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ = torch.ones([10,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_ = F.mse_loss(y_,t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(4.4101), (tensor([[[ 1.0484e-01,  1.0205e-03, -2.2222e-02],\n",
      "         [ 6.4675e-02, -2.2222e-02, -2.2222e-02],\n",
      "         [-1.5249e-02, -2.2222e-02,  1.5059e-02]],\n",
      "\n",
      "        [[-2.2222e-02,  4.1906e-02,  5.5603e-02],\n",
      "         [-2.8070e-04,  4.6955e-02,  1.5677e-02],\n",
      "         [-2.2222e-02, -2.2222e-02, -2.2222e-02]],\n",
      "\n",
      "        [[-2.2222e-02,  8.6899e-03, -7.0202e-03],\n",
      "         [ 2.1655e-02,  2.5630e-02, -2.2222e-02],\n",
      "         [ 1.2694e-03, -2.2222e-02, -2.2222e-02]],\n",
      "\n",
      "        [[ 9.7668e-02,  4.0748e-02, -2.2222e-02],\n",
      "         [-2.2222e-02, -2.2222e-02, -5.2652e-03],\n",
      "         [-1.9457e-02,  8.4247e-02, -2.2222e-02]],\n",
      "\n",
      "        [[ 2.3868e-02,  2.8454e-02, -2.2222e-02],\n",
      "         [ 2.2878e-01, -2.2222e-02, -2.2222e-02],\n",
      "         [-2.2222e-02, -5.9339e-03,  2.2161e-03]],\n",
      "\n",
      "        [[-2.2222e-02, -2.2222e-02, -2.2222e-02],\n",
      "         [-1.0118e-02, -2.2222e-02,  9.7206e-02],\n",
      "         [ 1.7443e-01,  1.0722e-01, -2.2222e-02]],\n",
      "\n",
      "        [[ 1.7372e-02, -2.2222e-02, -2.2222e-02],\n",
      "         [-2.2222e-02, -7.0795e-03, -2.2222e-02],\n",
      "         [-2.2222e-02,  6.2806e-02, -2.2222e-02]],\n",
      "\n",
      "        [[ 1.6780e-02, -2.2222e-02,  3.6242e-02],\n",
      "         [-2.2222e-02,  1.7055e-02, -2.2222e-02],\n",
      "         [ 1.9544e-02, -2.2222e-02,  5.4058e-02]],\n",
      "\n",
      "        [[ 7.8679e-02, -2.1411e-02, -1.5661e-04],\n",
      "         [-2.2222e-02, -2.2222e-02, -2.2222e-02],\n",
      "         [-1.8711e-02, -2.2222e-02, -2.2222e-02]],\n",
      "\n",
      "        [[-2.2222e-02,  4.3490e-02, -2.2222e-02],\n",
      "         [-5.9237e-03, -2.2222e-02, -2.2222e-02],\n",
      "         [ 8.2196e-02,  2.2114e-02, -2.2222e-02]]]), tensor([[[-1.0484e-01, -1.0205e-03,  2.2222e-02],\n",
      "         [-6.4675e-02,  2.2222e-02,  2.2222e-02],\n",
      "         [ 1.5249e-02,  2.2222e-02, -1.5059e-02]],\n",
      "\n",
      "        [[ 2.2222e-02, -4.1906e-02, -5.5603e-02],\n",
      "         [ 2.8070e-04, -4.6955e-02, -1.5677e-02],\n",
      "         [ 2.2222e-02,  2.2222e-02,  2.2222e-02]],\n",
      "\n",
      "        [[ 2.2222e-02, -8.6899e-03,  7.0202e-03],\n",
      "         [-2.1655e-02, -2.5630e-02,  2.2222e-02],\n",
      "         [-1.2694e-03,  2.2222e-02,  2.2222e-02]],\n",
      "\n",
      "        [[-9.7668e-02, -4.0748e-02,  2.2222e-02],\n",
      "         [ 2.2222e-02,  2.2222e-02,  5.2652e-03],\n",
      "         [ 1.9457e-02, -8.4247e-02,  2.2222e-02]],\n",
      "\n",
      "        [[-2.3868e-02, -2.8454e-02,  2.2222e-02],\n",
      "         [-2.2878e-01,  2.2222e-02,  2.2222e-02],\n",
      "         [ 2.2222e-02,  5.9339e-03, -2.2161e-03]],\n",
      "\n",
      "        [[ 2.2222e-02,  2.2222e-02,  2.2222e-02],\n",
      "         [ 1.0118e-02,  2.2222e-02, -9.7206e-02],\n",
      "         [-1.7443e-01, -1.0722e-01,  2.2222e-02]],\n",
      "\n",
      "        [[-1.7372e-02,  2.2222e-02,  2.2222e-02],\n",
      "         [ 2.2222e-02,  7.0795e-03,  2.2222e-02],\n",
      "         [ 2.2222e-02, -6.2806e-02,  2.2222e-02]],\n",
      "\n",
      "        [[-1.6780e-02,  2.2222e-02, -3.6242e-02],\n",
      "         [ 2.2222e-02, -1.7055e-02,  2.2222e-02],\n",
      "         [-1.9544e-02,  2.2222e-02, -5.4058e-02]],\n",
      "\n",
      "        [[-7.8679e-02,  2.1411e-02,  1.5661e-04],\n",
      "         [ 2.2222e-02,  2.2222e-02,  2.2222e-02],\n",
      "         [ 1.8711e-02,  2.2222e-02,  2.2222e-02]],\n",
      "\n",
      "        [[ 2.2222e-02, -4.3490e-02,  2.2222e-02],\n",
      "         [ 5.9237e-03,  2.2222e-02,  2.2222e-02],\n",
      "         [-8.2196e-02, -2.2114e-02,  2.2222e-02]]])))\n"
     ]
    }
   ],
   "source": [
    "print(vjp(func = F.mse_loss,inputs = (y_,t_)))\n",
    "#y_bar_ = vjp(func = F.mse_loss,inputs = (y_,t_))[1][0]\n",
    "#print(y_bar_)\n",
    "#print(y_bar_.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2.6546), (tensor([[ 6.2672e-03,  1.2650e-02, -1.9658e-02,  6.7980e-02,  6.4134e-02,\n",
      "          8.3596e-03, -1.3857e-02],\n",
      "        [ 9.9524e-03,  1.0229e-01, -4.2670e-03,  4.7433e-04, -1.3110e-02,\n",
      "          1.1048e-01,  1.1469e-01],\n",
      "        [ 1.7734e-01, -1.6613e-02,  6.5359e-02,  6.0111e-02, -6.5277e-02,\n",
      "          1.4835e-02, -9.0519e-03],\n",
      "        [-1.9503e-02,  8.7446e-03,  6.6337e-02, -7.2317e-03,  5.9847e-02,\n",
      "         -1.6922e-02, -4.7051e-02],\n",
      "        [-5.9387e-03,  3.4639e-02,  4.5419e-02, -4.2617e-03,  1.3630e-02,\n",
      "         -2.3614e-02,  5.1468e-02],\n",
      "        [ 4.3061e-03,  6.3390e-02,  2.2936e-02,  1.1105e-04, -2.5566e-02,\n",
      "          5.8967e-02,  2.7806e-02],\n",
      "        [-5.9983e-02, -6.0710e-03, -4.5048e-02,  2.1873e-02, -3.5698e-03,\n",
      "         -8.6097e-03,  5.3809e-02],\n",
      "        [-6.8750e-02,  3.9426e-02, -2.0179e-02, -4.0757e-02,  3.9555e-02,\n",
      "         -8.7818e-03,  3.1005e-02],\n",
      "        [ 8.2523e-04, -1.3954e-02,  3.3289e-02,  3.8400e-02,  5.1360e-02,\n",
      "          3.6493e-03,  5.5994e-02],\n",
      "        [ 1.1274e-02,  2.4691e-02,  5.2435e-02,  2.3992e-02,  3.1103e-02,\n",
      "         -2.4338e-02, -1.9820e-02]]), tensor([[-6.2672e-03, -1.2650e-02,  1.9658e-02, -6.7980e-02, -6.4134e-02,\n",
      "         -8.3596e-03,  1.3857e-02],\n",
      "        [-9.9524e-03, -1.0229e-01,  4.2670e-03, -4.7433e-04,  1.3110e-02,\n",
      "         -1.1048e-01, -1.1469e-01],\n",
      "        [-1.7734e-01,  1.6613e-02, -6.5359e-02, -6.0111e-02,  6.5277e-02,\n",
      "         -1.4835e-02,  9.0519e-03],\n",
      "        [ 1.9503e-02, -8.7446e-03, -6.6337e-02,  7.2317e-03, -5.9847e-02,\n",
      "          1.6922e-02,  4.7051e-02],\n",
      "        [ 5.9387e-03, -3.4639e-02, -4.5419e-02,  4.2617e-03, -1.3630e-02,\n",
      "          2.3614e-02, -5.1468e-02],\n",
      "        [-4.3061e-03, -6.3390e-02, -2.2936e-02, -1.1105e-04,  2.5566e-02,\n",
      "         -5.8967e-02, -2.7806e-02],\n",
      "        [ 5.9983e-02,  6.0710e-03,  4.5048e-02, -2.1873e-02,  3.5698e-03,\n",
      "          8.6097e-03, -5.3809e-02],\n",
      "        [ 6.8750e-02, -3.9426e-02,  2.0179e-02,  4.0757e-02, -3.9555e-02,\n",
      "          8.7818e-03, -3.1005e-02],\n",
      "        [-8.2523e-04,  1.3954e-02, -3.3289e-02, -3.8400e-02, -5.1360e-02,\n",
      "         -3.6493e-03, -5.5994e-02],\n",
      "        [-1.1274e-02, -2.4691e-02, -5.2435e-02, -2.3992e-02, -3.1103e-02,\n",
      "          2.4338e-02,  1.9820e-02]])))\n"
     ]
    }
   ],
   "source": [
    "# trying with 7 final output units\n",
    "\n",
    "_x = torch.randn([10,3],dtype= torch.float)\n",
    "_W = torch.randn([3,7],dtype= torch.float)\n",
    "_b = torch.randn([10,7], dtype = torch.float)\n",
    "\n",
    "_z = _x.matmul(_W) + _b\n",
    "_y = F.relu(_z)\n",
    "_t = torch.randn([10,7], dtype= torch.float)\n",
    "\n",
    "_y_bar = vjp(F.mse_loss,(_y,_t))\n",
    "print(_y_bar)\n",
    "\n",
    "#gradients are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(2.8841), (tensor([[0.7892],\n",
      "        [0.8819]]), tensor([-2.3133,  0.6422])))\n"
     ]
    }
   ],
   "source": [
    "_x = torch.randn([2,3],dtype= torch.float)\n",
    "_W = torch.randn([3,1],dtype= torch.float)\n",
    "_b = torch.randn([2,1], dtype = torch.float)\n",
    "\n",
    "_z = _x.matmul(_W) + _b\n",
    "_y = F.relu(_z)\n",
    "_t = torch.randn(2, dtype= torch.float) #Fail\n",
    "#_t = torch.randn([2,1], dtype= torch.float) #Pass . With exact shape it passes\n",
    "\n",
    "_y_bar = vjp(F.mse_loss,(_y,_t))\n",
    "print(_y_bar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5871, 1.1182], requires_grad=True)\n",
      "tensor([[0.],\n",
      "        [0.]], grad_fn=<ReluBackward0>)\n",
      "tensor([0.5871, 1.1182])\n",
      "tensor([[-0.8526],\n",
      "        [-0.8526]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akashe/anaconda3/envs/latest_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Alright I figured out the problem. when the shape of target variables is exactly same as to input then the gradients\n",
    "come out right. but lets say if the shape of target is [2] and input is [2,1] then gradients arent correct because\n",
    "there is another operation involved in the middle for reshaping.\n",
    "\n",
    "I WILL HAVE TO BE VERY CAREFUL WITH SHAPES COZ broadcasting CHANGES GRADIENT when used for calculating loss in \n",
    "MSE as mentioned in the user warning..so, TARGET SHUD ALWAYS BE OF SAME DIMENSION \n",
    "ALSO, VJP ONLY TAKES TENSORS AS INPUT\n",
    "\n",
    "Actually lets check the gradients using backward()\n",
    "'''\n",
    "\n",
    "#x = torch.rand(2)\n",
    "#print(vjp(torch.reshape,(x,(-1*torch.ones(1)).data)))\n",
    "\n",
    "_x = torch.randn([2,3],dtype= torch.float,requires_grad = True)\n",
    "_W = torch.randn([3,1],dtype= torch.float, requires_grad = True)\n",
    "_b = torch.randn([2,1], dtype = torch.float, requires_grad = True)\n",
    "\n",
    "_z = _x.matmul(_W) + _b\n",
    "_y = F.relu(_z)\n",
    "_y.retain_grad()\n",
    "_t = torch.randn(2,requires_grad=True) # Pass only with exact shape [2,1]\n",
    "\n",
    "\n",
    "_loss = F.mse_loss(_y,_t)\n",
    "_loss.backward()\n",
    "\n",
    "print(_t)\n",
    "print(_y)\n",
    "\n",
    "print(_t.grad)\n",
    "print(_y.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.3977), (tensor([[0.2783],\n",
      "        [0.8473]]), tensor([[-0.2783],\n",
      "        [-0.8473]]), tensor([[0.],\n",
      "        [0.]])))\n"
     ]
    }
   ],
   "source": [
    "_x = torch.randn([2,3],dtype= torch.float)\n",
    "_W = torch.randn([3,1],dtype= torch.float)\n",
    "_b = torch.randn([2,1], dtype = torch.float)\n",
    "\n",
    "_z = _x.matmul(_W) + _b\n",
    "_y = F.relu(_z)\n",
    "_t = torch.randn([2,1], dtype= torch.float)\n",
    "_r = torch.randn([2,1], dtype= torch.float)\n",
    "\n",
    "def loss(y,t,r):\n",
    "    return torch.sum(torch.mul((y-t),(y-t)))/len(y)\n",
    "\n",
    "_y_bar = vjp(func=loss,inputs=(_y,_t,_r))\n",
    "print(_y_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1.4710), (tensor([[0.4532],\n",
      "        [0.0565]]), tensor([[1.6381],\n",
      "        [0.1048]]), tensor([[-0.6377, -0.4361, -0.9107],\n",
      "        [ 0.0015, -0.0216, -0.0043]]), tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akashe/anaconda3/envs/latest_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([2, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_x = torch.randn([2,3],dtype= torch.float)\n",
    "_W = torch.randn([3,1],dtype= torch.float)\n",
    "_b = torch.randn([2,1], dtype = torch.float)\n",
    "\n",
    "_z = _x.matmul(_W) + _b\n",
    "_y = F.relu(_z)\n",
    "_t = torch.randn([2,1], dtype= torch.float) \n",
    "\n",
    "def dummy(y,t,*args):\n",
    "    y = y*args[0]\n",
    "    return F.mse_loss(y,t)\n",
    "\n",
    "_y_bar = vjp(dummy,(_y,_t,_x,_W))\n",
    "print(_y_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
