{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import vjp\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Testing designs for Module class:\n",
    "\n",
    "Important requirements:\n",
    "1)class should support architectures of wierd shapes and even circular ones. It shouldnt be restricted to sequential\n",
    "    layers.\n",
    "2) each module has 2 important functions:\n",
    "    a) forward pass\n",
    "    b) backward pass\n",
    "        - gradients of outputs wrt inputs\n",
    "        - gradients of outputs wrt parameters \n",
    "3) should support any number of inputs to a module?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    '''\n",
    "    The idea behind creating this class is to keep track of\n",
    "    parents of lone tensor between modules\n",
    "    Will prune once final backward is implemented\n",
    "    \n",
    "    Each alone node should have only 1 parent\n",
    "    Each module can have multiple parent nodes\n",
    "    Each node/tensor is called only one time(mybe used by muliple child) ..may result in huge model sizes coz it \n",
    "    stops reuse of tensor\n",
    "    '''\n",
    "    def __init__(self,o):\n",
    "        self.parent = None\n",
    "        self.child = []\n",
    "        self.o = o\n",
    "        self.outgoing_gradient = []\n",
    "        self.output_order = None\n",
    "        self.pass_number = None\n",
    "        \n",
    "    def append_child(self,n):\n",
    "        self.child.append(n)\n",
    "        \n",
    "    def backward(self,gradient):\n",
    "        # if gradients from all children have arrived then sum them and call parents backward based \n",
    "        # on ouput ordering from the parent\n",
    "        # dont need self object of the child\n",
    "        assert self.o.size() == gradient.size()\n",
    "        self.outgoing_gradient.append(gradient)\n",
    "        \n",
    "        if len(self.child) == len(self.outgoing_gradient):\n",
    "            v = torch.stack(self.outgoing_gradient,dim =0).sum(dim = 0)\n",
    "            self.parent.backward(v,self.output_order,self.pass_number)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # if implemented need to change loop in Module.__call__\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Module():\n",
    "    '''\n",
    "    Two usage constraints till now:\n",
    "    1) keeping *args in forward def\n",
    "    2) using class.__call__ instead of forward where we actually define forward pass\n",
    "    3) only inputs and targets can be defined as tensors. Every other transformation\n",
    "        even a simple (learnable)matrix mul has to be done using module.\n",
    "    4) all outputs should later be used. Eg if module ouputs a,b,c; all three should later be used in other modules\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.pass = 0 # keeps track of how many times the modules is passed through\n",
    "#         self.node = Node(self)\n",
    "        self.parents = []\n",
    "        \n",
    "        # for each pass\n",
    "        self.inputs = [] # do I need this? Yes for gradients\n",
    "        self.outputs = []\n",
    "#         self.trainable_params = [] #clone of params at that point\n",
    "        self.gradients_from_output = [] #will have to mantain sequence here\n",
    "        self.output_nodes = []\n",
    "        self.gradients_for_trainable_params = [] #sum of this list will be the gradients for trainable params\n",
    "        \n",
    "        '''\n",
    "        # Save cloned values of all tensors used in forward()???? Do I need this?? check bptt.\n",
    "        # I dont need saved tensors in forward as long as I am using single loss function.\n",
    "        '''\n",
    "        \n",
    "    def __call__(self,*input):\n",
    "        inputs_for_forward = []\n",
    "        parents_ = []\n",
    "        if hasattr(input,'__iter__') and not torch.is_tensor(input):\n",
    "            # multiple inputs\n",
    "            for i in input:\n",
    "                # Make sure i is a tensor or a node\n",
    "                if isinstance(i,Node) or torch.is_tensor(i):\n",
    "                    parents_.append(i)\n",
    "                    if not torch.is_tensor(i):\n",
    "                        i.append_child(self)\n",
    "                        self.inputs_for_forward.append(i.o)\n",
    "                    else:\n",
    "                        self.inputs_for_forward.append(i)\n",
    "                else:\n",
    "                    print(\" error : inputs should only be tensors or instances of class Node\")\n",
    "                    sys.exit(1)\n",
    "                    #TODO : make new exception\n",
    "        else:\n",
    "            #single input... Not needed?? input will always come as a list or tensor\n",
    "            if isinstance(input,Node) or torch.is_tensor(input):\n",
    "                parents_.append(input)\n",
    "                if not torch.is_tensor(input):\n",
    "                    input.append_child(self)\n",
    "                    self.inputs_for_forward.append(input.o)\n",
    "                else: \n",
    "                    self.inputs_for_forward.append(input)\n",
    "            else:\n",
    "                print(\" error : inputs should only be tensors or instances of class Node\")\n",
    "                sys.exit(1)\n",
    "        \n",
    "        outputs_= self.forward(*inputs_for_forward) # a simple trick to unlist a list \n",
    "        \n",
    "        output_node = []\n",
    "        \n",
    "        #Outputs_should alway be a single or multiple tensor\n",
    "        try:\n",
    "            if len(outputs_) and not torch.is_tensor(outputs_):\n",
    "                for j,i in enumerate(outputs_):\n",
    "                    assert torch.is_tensor(i)\n",
    "                    c = Node(i)\n",
    "                    c.parent = self\n",
    "                    c.output_order =j\n",
    "                    c.pass_number = self.pass\n",
    "                    output_node.append(c)\n",
    "            else:\n",
    "                assert torch.is_tensor(outputs_)\n",
    "                c = Node(outputs_)\n",
    "                c.parent = self\n",
    "                c.output_order = 0\n",
    "                c.pass_number = self.pass\n",
    "                output_node.append(c)\n",
    "        except TypeError:\n",
    "            print(\" Only lists or tuples of tensors allowed as output of forward()\")\n",
    "        \n",
    "        self.inputs.append(inputs_for_forward)\n",
    "        self.outputs.append(outputs_)\n",
    "        self.output_nodes.append(output_node)\n",
    "        self.gradients_from_output.append([None]*len(output_node))\n",
    "        self.parents.append(parents_)\n",
    "        self.pass += 1\n",
    "        \n",
    "    def forward(self,input,*args): # will have to pass by reference\n",
    "        '''\n",
    "        while implementing I have in child classes\n",
    "        I have to keep the func def like this\n",
    "        forward(input_1,input_2..input_n,*args)\n",
    "        where\n",
    "        input_1...input_n are the number of inputs expected\n",
    "        *args for self trainable tensors that need gradients\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_trainable_params(self):\n",
    "        # I dont have to worry about pass by assignment coz vjp just requires param values\n",
    "        # once I get gradients I can just update the params in the same order with gradient\n",
    "        trainable_params = []\n",
    "        for i in vars(self):\n",
    "            if torch.is_tensor(self.__getattribute__(i)):\n",
    "                if self.__getattribute__(i).requires_grad == True:\n",
    "                   trainable_params.append(self.__getattribute__(i))\n",
    "        return trainable_params\n",
    "    \n",
    "    \n",
    "    def update_parameters(self,gradients):\n",
    "        # use getattribute again for updating \n",
    "        # same order of iteration over dicts in python 3.6+\n",
    "        \n",
    "        # update of params occurs when u have gradients from all the passes.\n",
    "        # you sum those gradients and update parameter with it\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def prepare_gradients_for_trainable_params(self, gradients):\n",
    "        # Do I need pass_no info here..not exactly.cud be good to check\n",
    "        # when the length of the list becomes equal to self.pass_no then update the variable\n",
    "        self.gradients_for_trainable_params.append(gradients)\n",
    "        if len(self.gradients_for_trainable_params)==self.pass:\n",
    "            self.update_parameters()\n",
    "    \n",
    "    def make_tuple_for_vjp(self):\n",
    "        pass\n",
    "    \n",
    "    def backward(self,v,output_order,pass_no):\n",
    "        '''\n",
    "        Assumption all output nodes are later used and are involved in gradients\n",
    "        ouput nodes do only one backward with a prticular pass no\n",
    "        TODO: update method for Nodes with no child\n",
    "        '''\n",
    "        \n",
    "        self.gradients_from_output[pass_no][output_order] = v\n",
    "        '''\n",
    "        check if gradients from all child of the pass no are here then do backwards for its parents and send back\n",
    "        gradients with respect to inputs and save gradients wrt to params:\n",
    "        From the modular approach u can consider delta(i+1) as a sum of gradients from previous layer \n",
    "        So I am thinking that we can send gradients in steps instead of sending them as one coz that will \n",
    "        prevent circular architectures and same modules having different inputs at different times\n",
    "        '''\n",
    "        # checking gradients from all child present\n",
    "        if not self.gradients_from_output[pass_no].__contains__(None):\n",
    "            #calculate gradient wrt to input and trainable params\n",
    "            trainable_params = self.get_trainable_params()\n",
    "            output_, gradients = vjp(self.forward,(*self.inputs[pass_no],*trainable_params),*self.gradients_from_output[pass_no])\n",
    "            \n",
    "            gradients_for_inputs = gradients[:len(self.inputs[pass_no])]\n",
    "            gradients_for_params = gradients[len(self.inputs[pass_no]):]\n",
    "\n",
    "            # call backward on parent nodes..check if parent is a tensor\n",
    "            # len of gradients of input is same as the number of parents for that pass\n",
    "            assert len(gradients_for_inputs)== len(self.parents[pass_no])\n",
    "            for i,j in zip(self.parents[pass_no],gradients_for_inputs):\n",
    "                if not torch.is_tensor(i):\n",
    "                    # not passing gradients to input variable [Remember assumption only input variables are plain \n",
    "                    # tensors rest all intermediary tensors are nodes]\n",
    "                    i.backward(j)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1508])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to assign parents to intermediate tensors between modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parents\n",
      "child\n",
      "a\n",
      "b\n",
      "c\n",
      "[tensor([-1.0955], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class A:\n",
    "    def __init__(self):\n",
    "        _x = torch.randn([2,3],dtype= torch.float)\n",
    "        _W = torch.randn([3,1],dtype= torch.float)\n",
    "        _b = torch.randn([2,1], dtype = torch.float)\n",
    "        _t = torch.randn([2,1], dtype= torch.float)\n",
    "        _r = torch.randn([2,1], dtype= torch.float)\n",
    "        \n",
    "    def forward(self):\n",
    "        _z = _x.matmul(_W) + _b\n",
    "        _y = F.relu(_z)\n",
    "        return F.mse_loss(y,t)\n",
    "    \n",
    "    def backward():\n",
    "        print(vjp(forward,self)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 30)\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    return 10,20,30\n",
    "\n",
    "a = test()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 10, 20)\n",
      "15\n",
      "30\n",
      "35\n",
      "[<function <genexpr>.<lambda> at 0x7faf6a6a1b70>, <function <genexpr>.<lambda> at 0x7faf6a6a1ae8>, <function <genexpr>.<lambda> at 0x7faf6a6a1c80>, <function <genexpr>.<lambda> at 0x7faf6a5f0048>]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type or tuple of types",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ca3633e6832b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: isinstance() arg 2 must be a type or tuple of types"
     ]
    }
   ],
   "source": [
    "class A():\n",
    "    def __init__(self):\n",
    "        self.no_of_inputs = 2\n",
    "    \n",
    "    def test(self,*args):\n",
    "        print(args)\n",
    "#         return c+e # this fails coz e wasnt sent in params\n",
    "        return c + b # but this works!! this a nice little trick\n",
    "        \n",
    "    def test2(self,*args):\n",
    "        self.test3(*args)\n",
    "    \n",
    "    def test3(self,a , b ,*args):\n",
    "        print(*args)\n",
    "        print(a+b)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "# class B(A):\n",
    "#     def test()\n",
    "\n",
    "a = A()\n",
    "c = 10\n",
    "b = 5\n",
    "print(a.test(c,b,10,20))\n",
    "\n",
    "x = A()\n",
    "x.test2(15,20,30)\n",
    "\n",
    "m = [1,2,3,4]\n",
    "x = ( lambda : i for i in m)\n",
    "print(list(x))\n",
    "\n",
    "isinstance(x,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 30)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8622b4100357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "def f():\n",
    "    return 10,20,30\n",
    "h = f()\n",
    "print(h)\n",
    "r = 10\n",
    "print(len(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Base:\n",
    "    def __init__(self):\n",
    "        self.trainable_params ={}\n",
    "\n",
    "class A(Base):\n",
    "    def __init__(self,a,b):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        \n",
    "    def __setattr__(self,k,v):\n",
    "        if torch.is_tensor(v) and v.requires_grad == True:\n",
    "            self.trainable_params[k]=v\n",
    "        super().__setattr__(k,v)\n",
    "        \n",
    "    def forward(self,input,**kwargs):\n",
    "        print(kwargs)\n",
    "        print(locals())\n",
    "        \n",
    "    \n",
    "    def backward(self):\n",
    "        self.forward(torch.randn(1),**self.trainable_params)\n",
    "        \n",
    "    \n",
    "a =A(torch.randn(1,requires_grad=True),torch.randn(1,requires_grad=True))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tensor([0.6448], requires_grad=True), 'b': tensor([0.7502], requires_grad=True)}\n",
      "{'self': <__main__.A object at 0x7fd78ce823d0>, 'input': tensor([0.4845]), 'kwargs': {'a': tensor([0.6448], requires_grad=True), 'b': tensor([0.7502], requires_grad=True)}}\n"
     ]
    }
   ],
   "source": [
    "a.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
