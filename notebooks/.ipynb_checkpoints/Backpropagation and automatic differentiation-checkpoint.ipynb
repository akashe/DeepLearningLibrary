{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation and automatic differentiation:\n",
    "\n",
    "[My purpose: To dump all that I learnt. To make/express it the best way I can]\n",
    "Few months ago I was trying to understand how learning actually happens and it struck me that I understood backpropagation in a very shallow way. So, I decided to understand it in depth and see how it is actually implemented in big neural networks. It will be a long post. Its not intended as quick read to understand backprop. We will be looking into it in depth and I hope you find the article useful. \n",
    "\n",
    "Notes:\n",
    "1) the articles may seem quite math heavy but it isnt so. I have tried to use animation to ease the understanding.\n",
    "2) I was trying to make a library of my own that does backprop and it wasnt easy to find all the necessary information at one place. So this a compilation of things one would need to implement or understand backprop deeply.\n",
    "3) Most articles explain with a simple architecture. I use 2 layer MLP and use the same example throughout so as to enable extrapolation to bigger architectures.\n",
    "\n",
    "### Intro to backprop:\n",
    "Central to the concept of backprop is the idea of a cost function. Cost functions let us choose what we want to achieve. Either we want to maximize all the rewards or we want to minimize our errors. Cost function helps to formalize the uncertainty of our models[check Appendix A] and reduce it.\n",
    "\n",
    "Let's say our model consists of parameters $\\theta$ and our cost function is $\\mathcal{J}(\\theta)$. So, if we have the gradients $\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\theta}$ then we can update the parameters $\\theta$ using gradient descent[For more on gradients and gradient descent check Appendix B].\n",
    "\n",
    "A forward pass in a simple Multilayer Preceptron looks like this:\n",
    "\n",
    "![Forward pass](https://raw.githubusercontent.com/akashe/gifsandvids/main/forward_pass.gif)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The above model has these parameter $(W_1,b_1,W_2,b_2,W_3,b_3,W_4,b_4)$. A partial derivative of $W_1$ wrt to the cost function tells how simple changes in $W_1$ changes the value of $\\mathcal{J}(\\theta)$. But here lies a catch. $W_1$ doesn't directly affect the value of cost function. Value of $W_1$ affects the value of output of node 1. This output value later affects output of Node 3 and 4. Node 3 and 4 directly affect $\\mathcal{J}(\\theta)$. As this gif shows:\n",
    "\n",
    "![Gradients for 1 param](https://raw.githubusercontent.com/akashe/gifsandvids/main/gradients.gif)\n",
    "\n",
    "You can think of this as information paths. All the paths that connect a parameter to a cost function contribute in the total derivative of that parameter wrt cost function.In the above case. Since there are 2 paths that connect $W_1$ with $\\mathcal{J}(\\theta)$, $\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial W_1}$ has two terms:\n",
    "\\begin{aligned}\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial W_1} &= \\text{derivative from path 1} + \\text{derivative from path 2} \\\\\n",
    "&= \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial Z_5}\\frac{\\partial Z_5}{\\partial Z_3}\\frac{\\partial Z_3}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_1} + \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial Z_5}\\frac{\\partial Z_5}{\\partial Z_4}\\frac{\\partial Z_4}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_1}\n",
    "\\end{aligned}\n",
    "\n",
    "where $Z_i$ is the output of the ith node and considering Node O as Node 5. We got these expressions using Chain Rule[check Appendix C]\n",
    "\n",
    "If you notice the gradients of our model parameters, you will notice two main things:\n",
    "1. To update the value of $W_1$ we have to save 6 jacobians. For a much deeper network, the number of jacobians we have to save will keep on increasing. It is a big problem because jacobian can very large[Check appendix D].\n",
    "2. Repitions of jacobians: If you check the next gif, you will notice how so many terms are repeated and need to be calculated again and again.\n",
    "![repition in Gradients](https://raw.githubusercontent.com/akashe/gifsandvids/main/patterns_in_gradients.gif)\n",
    "          \n",
    "\n",
    "### Automatic Differentiation:\n",
    "Practically we calculate gradients using Automatic Differentiation or Autodiff. It is used in almost all libraries that support Neural Networks because it circumvents the aforementioned problems. Autodiff achieves so by leveraging the computational graph(CG) and converting all operations into primitive operation. \n",
    "\n",
    "1. #### Converting Computational graph into primitive operations:\n",
    "    This allows us to reformulate the entire CG in terms of simple functions and basic operations like $exp(),tanh(),matmul(), addition(),substraction(), division()$ etc. The process of converting CG into primitive operations is done behind the scenes in Pytorch and Tensorflow. The reason why its done will soon become clear.\n",
    "    \n",
    "    We can rewrite operations in our Node 1 as:\n",
    "    \\begin{aligned}t3&=ReLU(t2)\\\\t2&=t1+b_1\\\\ t1 &= I*W_1\\end{aligned}\n",
    "where,$t_i$ are intermediate nodes with results of primitive operations. \n",
    "\n",
    "    Similarly we can rewrite the CG for entire architecture as:\n",
    "![CG to Primitive operations](https://raw.githubusercontent.com/akashe/gifsandvids/main/Computational%20graph%20to%20Primitive%20operations.jpg)\n",
    "\n",
    "2. #### Traversing the CG in reverse order:\n",
    "   First off, we use a different notation in autodiff. We represent $\\frac{ \\partial \\mathcal {J}(\\theta)}{\\partial x}$ as $\\bar{x}$. Autodiff algorithm can be wriiten as:\n",
    "   - Find value of all primitive nodes in the forward pass.\n",
    "   - In backward pass: Assign last node with a gradient of 1. Iterate through CG in reverse order. Assign each node a gradient $\\overline{v_i}$ such that:\n",
    "   \\begin{equation}\\overline{v_i} = \\sum_{j}{} \\overline{v_j}\\frac{\\partial v_j}{\\partial v_i}\\end{equation}\n",
    "       where, j are all nodes that use value of Node i in forward pass. This is similar to chain rule we discussed above.\n",
    "   \n",
    "    We shall now try it on the above CG. \n",
    "   ![Gradients in Autodiff Explanation](https://raw.githubusercontent.com/akashe/gifsandvids/main/gradients%20in%20autodiff%20explnation.gif)\n",
    "   If you want to check out gradients of all parameters in the above example check out Appendix E.\n",
    "   \n",
    "   Key properties:\n",
    "   \n",
    "   1. In autodiff we are iteratively applying chain rule on each node as opposed to finding chain rule expression for each node.\n",
    "   2. Since gradients are passed down there is no repitition in terms.\n",
    "   3. If you notice, all gradients are of the form $\\overline{v_y}*\\frac{\\partial y}{\\partial x}$. This avoids costly multiplication of huge jacobians.\n",
    "   4. Overall cost of doing backpropagation depends linearly on time taken to do forward pass.\n",
    "    \n",
    "3. #### Writing Jacobian Products for primitive operation:\n",
    "    We still havent answered why we reformulated our CG down to primitive operations. There are 2 main reasons:\n",
    "    \n",
    "    1. We can bypass construction of Jacobians: As you noticed that in autodiff, the gradients are of the form $\\overline{v_y}*\\frac{\\partial y}{\\partial x}$, Mutliplication with Jacobian can still be costly. If we break down our in CG into only user-defined primitive functions like $add(),substract(),matmul(),divide(),exp() etc$ then we can directly mention the values of jacobians in the functions. Such functions are called VJP(Vector Jacobian Product) or sometimes JVP(Jacobian Vector product). They directly compute the gradients for a particular primitive node. Eg:\n",
    "    \n",
    "        1. Lets take a simple primitive node which does addition:\n",
    "        \n",
    "        $z= x + y, \\text{  }\\overline{v}$ is gradient coming from its child. Then we write a simple function\n",
    "        \n",
    "        $def\\text{  }AdditionVjp(x,y,z,\\overline{v})\\{\\text{  } return \\text{  }\\overline{v}\\text{  }\\}$ \n",
    "        \n",
    "        So $\\overline{z}=AdditionVjp(x,y,z,\\overline{v})$\n",
    "        2. Primitive node that does exp():\n",
    "        \n",
    "        $z=e^{x},\\text{  }\\overline{v}$ is gradient coming from its child. Then its VJP:\n",
    "        \n",
    "        $def\\text{  }ExponentiationVjp(x,z,\\overline{v})\\{\\text{  } return \\text{  }\\overline{v}*z\\text{  }\\}$\n",
    "        \n",
    "        So $\\overline{z}=ExponentiationVjp(x,z,\\overline{v})$\n",
    "        3. Primitive node that does log():\n",
    "        \n",
    "        $z=\\log{x},\\text{  }\\overline{v}$ is gradient coming from its child. Then its VJP:\n",
    "        \n",
    "        $def\\text{  }LogarithmVjp(x,z,\\overline{v})\\{\\text{  } return \\text{  }\\overline{v}/x\\text{  }\\}$\n",
    "        \n",
    "        So $\\overline{z}=ExponentiationVjp(x,z,\\overline{v})$\n",
    "        \n",
    "    I hope you understand how defining VJP for these primitive function avoids creating a Jacobian and also directly returns gradient wrt to that node thereby avoiding multiplication with a Jacobian.\n",
    "    \n",
    "    2. If we can break down a CG to its primitive operations and define corresponding VJP, we have automated backpropagation. Yaaayyyyy. Well there are many more things we have to take care of. Creating strucutres that enable identification of primitive operation, also in real world application every thing is done through matrices so we will have to take care of matrix calculus also. But the ability to only worry about forward pass and let your library take care of the rest is a huge boost to productivity.  \n",
    " \n",
    "\n",
    "\n",
    "### Message passing view of backprop\n",
    "In this section we will see how to leverage the recursive nature of backpropagation. Due to this property we can design architectures containing any number of modules arranged in any shape as long as these modules follow certain conditions. The backpropagation signal here is similar to $\\overline{v_i}$ which is passed between parent and child nodes.\n",
    "1. #### Modular view of architectures:\n",
    "    We can formulate our architectures as interconnected modules. Each module has 4 types of signal passing through it:\n",
    "        1. Input($z_{in}$)\n",
    "        2. Ouput($z_{out}$): A module processes Input and transforms it to give an Output\n",
    "        3. Incoming Gradients($\\delta_{in}$): These are gradients coming from children of this module during backpropagation.\n",
    "        4. Outgoing Gradients($\\delta_{out}$): These are gradients that the module sends to its parent.\n",
    "        \n",
    "    A module should atleast have these 3 functions:\n",
    "        1. forward(): that converts inputs to outputs of a module\n",
    "        2. backward() that finds gradients of outputs wrt to inputs($\\delta_{out}$) of the module\n",
    "        3. parameter_Gradients(): which finds gradients of outputs wrt trainable parameters of the module. \n",
    "    \n",
    "    And that's it. Now you can rearrange these modules in desired shape to form any architectures that wants to use backprop. However we still have not discussed how to formulate backward() and parameter_Gradients() in case of multiple parents and guess what we will use. Yes our old pal, Chain Rule.\n",
    "    \n",
    "    gif of a single module containing functions and signals..then zomming out and showing a a connected whacky network if possible. \n",
    "\n",
    "2. #### Gradient flow between different modules:\n",
    "    The end expression is same in case of single or multiple child. But to build intuition lets start with a single child.\n",
    "    1. Single child:\n",
    "        Let our current module be $M_i$ and its child is $M_j$. Then we have only 1 incoming gradient. $\\delta_{in}^{j}$. Then for the functions:\n",
    "        1. backward(): Remember backward() find gradients of outputs wrt to inputs($\\delta_{out}$). So from chain rule:\n",
    "        \\begin{equation} \\delta_{out}^{i} = \\delta_{out}^{j}\\frac{\\partial z_{in}^{j}}{\\partial z_{in}^{i}}\\end{equation}\n",
    "        That is the gradients that we send back are gradient of module output wrt to its inputs times the gradients we recieved.\n",
    "        2. parameter_Gradients(): Lets say our module has $\\theta_i$ as trainable parameters. Then gradient of Cost Function $\\mathcal{J}(\\theta)$ wrt to $\\theta_i$ is:\n",
    "        \\begin{equation}\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\theta_i} = \\delta_{out}^{j}\\frac{\\partial z_{in}^{j}}{\\partial \\theta_i}\\end{equation}\n",
    "    2. Mutiple child:\n",
    "        *In case of multiple child we have to accomdate for gradients recieved from all the children. The terms we formulated for a single child become summation over all the children.* Let's say module $M_i$ has N children then\n",
    "        1. backward() : \n",
    "        \\begin{equation} \\delta_{out}^{i} = \\sum_{j \\in N}\\delta_{out}^{j}\\frac{\\partial z_{in}^{j}}{\\partial z_{in}^{i}}\\end{equation}\n",
    "        i.e. sum of gradients coming from child times derivative of input to that child wrt to input of Module i\n",
    "        2. parameter_Gradients():\n",
    "        \\begin{equation}\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\theta_i} = \\sum_{j \\in N}\\delta_{out}^{j}\\frac{\\partial z_{in}^{j}}{\\partial \\theta_i}\\end{equation}\n",
    "        i.e. sum of gradients coming from child times derivative of input to that child wrt params of Module i\n",
    "\n",
    "\n",
    "\n",
    "In some way autodiff implements the module structure we discussed its just that each primitive operation itself becomes a module.\n",
    "I \n",
    "    \n",
    "Resources:\n",
    "\n",
    "Appendix:\n",
    "\n",
    "### Appendix A: Cost Functions\n",
    "To understand the need for a cost function we need to understand what learning is. In context of machine learning and deep learning algorithms, learning means ability to predict. In our models we want to reduce the uncertainity involved in our predictions or our learning of the data. We want our predictions to be spot on. We measure uncertainty using entropy.\n",
    "\n",
    "The measure of uncertainty is called entropy. By definition\n",
    "\n",
    "\\begin{equation*} \\mathcal{E} = - \\sum \\mathcal{P}(x) ln \\mathcal{P}(x) \\end{equation*}\n",
    "\n",
    "Lets see how cost functions reduce entropy of predictions for different tasks.\n",
    "\n",
    "#### For classification tasks:\n",
    "Lets task a classification task of n classes using softmax. \n",
    "So, the probability that a particular sample $x$ is of class n is:\n",
    "\n",
    "\\begin{equation*} \\mathcal{P}(x = n) = [\\frac{e^{k_n}}{\\sum_{i}^{n} e^{k_j}}]^{\\Omega_n} \\end{equation*}\n",
    "where, $\\Omega_n = \\begin{cases}\n",
    "1 &\\text{if } \\bar{y} \\text{=1 i.e. label for the class n is 1 ,else} \\\\\n",
    "0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "So, our likelihood for one data sample is:\n",
    "\\begin{equation*} \\mathcal{P}(x) = \\prod_{i=0}^{n}[\\frac{e^{k_n}}{\\sum_{j}^{n} e^{k_j}}]^{\\Omega_i} \\end{equation*}\n",
    " \n",
    "So entropy of classification for one data sample becomes:\n",
    "\n",
    "\\begin{equation*} \\mathcal{E} = - \\sum_{i=0}^{n} [\\frac{e^{k_n}}{\\sum_{j}^{n} e^{k_j}}]^{\\Omega_i} {\\Omega_i} \\ln [\\frac{e^{k_n}}{\\sum_{j}^{n} e^{k_j}}] \\end{equation*}\n",
    "\n",
    "The above expression may seem daunting but if you notice $\\Omega_i$ is 0 for all non label classes and when $\\Omega_i$ is 1 then the above expression gets it lowest value(0) when the model gives the probablility of 1 to the label class which will be a spot on prediction for the label class.\n",
    "\n",
    "#### For regression tasks:\n",
    "The first go to option for regression tasks is MeanSquarredError. But in MSE there is no notion of a $\\mathcal{P}(y)$. So we assume that by predicting $y$ we are learning a distribution of the logits. A go-to choice is to learn a normal distribution of our logits. We assume that are logits are the mean of that distribution and we find the $\\mathcal{P}(\\bar{y})$ using the probability density function(PDF) of the distribution. The idea is then to maximize the probabilities $\\mathcal{P}(\\bar{y})$ which happens when $\\bar{y} = y$.\n",
    "\n",
    "Using PDF of a normal distribution,\n",
    "\\begin{equation*} \\mathcal{P}(x) = \\frac{e^{-(\\bar{y} - y)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\end{equation*}\n",
    "\n",
    "So our entropy becomes,\n",
    "\\begin{align*} \\mathcal{E} &= - \\frac{e^{-^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}(-(\\bar{y} - y)^{2}/(2\\sigma^{2})- \\ln(\\sigma\\sqrt{2\\pi})) \\\\\n",
    "&\\propto  \\frac{e^{-^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}((\\bar{y} - y)^{2})\n",
    "\\end{align*}\n",
    "\n",
    "Again, a daunting expression but it minimizes when $(\\bar{y} = y)$\n",
    "\n",
    "So, as we saw in both cases, the point of a cost function is to measure the uncertainty of the model and reduce it.\n",
    "\n",
    "### Appendix B: Gradients & Gradient Descent\n",
    "A gradient tells how a function is changing at that particular point. Key points regarding gradients:\n",
    "1. A gradient shows the direction in which the function is increasing.\n",
    "2. When gradients are zero it means our function has reached a peak or a trough.\n",
    "\n",
    "If we take a small step towards the direction mentioned by a gradient, we move to a point with function value greater than the previous position. If we take this step again and again we reach a 'maxima' point of that function. This is the basis of many optimization alogrithms. Find gradients wrt to cost function and keep updating parameters in the direction of their gradients.\n",
    "\n",
    "#### Gradient Descent:\n",
    "Gradient Descent is one such optimization algorithm. It has very simple rule:\n",
    "\\begin{equation}\\theta_{i+1} = \\theta_i + \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta}  \\end{equation}\n",
    "where $\\alpha$ is the step size which regulates the amount of movement in the direction of gradient.\n",
    "We use the above update rule when we have maximize our cost function. To minimize our cost function we have to move in the opposite direction of our gradients. So the update rule becomes:\n",
    "\\begin{equation}\\theta_{i+1} = \\theta_i - \\alpha\\frac{\\partial J(\\theta)}{\\partial \\theta}  \\end{equation}\n",
    "\n",
    "#### Stochastic Gradient Descent:\n",
    "As simple as Gradient descent seems, implementing it for huge datasets has its own problem. Why? The problems remains in the gradient $\\frac{\\partial J(\\theta)}{\\partial \\theta}$. Let me explain, for most applications the cost function is defined something like this: \n",
    "\\begin{equation} J(\\theta) = \\sum_{i=0}^{n} f(\\theta,i)\\quad \\text{where, n is the total length of the dataset}\\end{equation}\n",
    "If we had a dataset of length 1000, then we find $f(\\theta,i)$ for each instance and then sum these to get $J(\\theta)$. We then use $J(\\theta)$ to find $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ and perform one optimization step for the entire dataset. Herein lies the problem, *Gradient Descent performs one optimization step for the entire length of dataset*. If we had a dataset of length 1 billion, we will have to iterate over those 1 billion instances before we can move once in the direction of gradients.\n",
    "\n",
    "Stochastic Gradient Descent comes here for rescue. SGD basically says, we can estimate the true gradient using gradient at each instance and if we estimate for a large number of instances we can come pretty close to true gradients. This allows working with batches when we have huge datasets. \n",
    "\n",
    "Note: Gradient descent will take you directly to maxima or minima. SGD will wander here and there but if repeated correctly, over time it will take you to the same place or atleast some place near it.\n",
    "\n",
    "### Appendix C: Chain Rule:\n",
    "You can actutally think of the Cost function as a function of functions. Using notation $Z_i$ as the output of node i and considering output node as node 5, we can say:\n",
    "\\begin{aligned}\n",
    "\\mathcal{J}(\\theta) &= f(\\bar{y},Z_5)\\\\\n",
    "&= f(\\bar{y},g(Z_3,Z_4)) \\\\\n",
    "&= f(\\bar{y},g(h(Z_1,Z_2),h(Z_1,Z_2)))\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "here $Z_1 = ReLU(I*W_1 +b_1) and Z_2 = ReLU(I*W_2 +b_2)$\n",
    "\n",
    "\n",
    "Chain Rule helps in finding gradients of composit functions such as $\\mathcal{J}(\\theta)$. Multivariate chain rule says,\n",
    "\\begin{equation} \\frac{\\partial}{\\partial t}f(x(t),y(t))= \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial t}\\end{equation}\n",
    "\n",
    "Now we can use the chain rule to find $\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial W_1}$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial W_1} &= \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\bar{y}}\\frac{\\partial \\bar{y}}{\\partial W_1} + \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial Z_5}\\frac{\\partial Z_5}{\\partial W_1} \\quad \\quad \\text{expanding } \\frac{\\partial Z_5}{\\partial W_1}\\\\\n",
    "&= 0 + \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial Z_5} ( \\frac{\\partial Z_5}{\\partial Z_3}\\frac{\\partial Z_3}{\\partial W_1} + \\frac{\\partial Z_5}{\\partial Z_4}\\frac{\\partial Z_4}{\\partial W_1}) \\quad \\text{expanding } \\frac{\\partial Z_3}{\\partial W_1} \\text{ and } \\frac{\\partial Z_4}{\\partial W_1}\\\\\n",
    "&= \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial Z_5}(\\frac{\\partial Z_5}{\\partial Z_3}(\\frac{\\partial Z_3}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_1} + \\frac{\\partial Z_3}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial W_1} ) +\\frac{\\partial Z_5}{\\partial Z_4}(\\frac{\\partial Z_4}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_1} + \\frac{\\partial Z_4}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial W_1}) )\\\\\n",
    "&= \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial Z_5}(\\frac{\\partial Z_5}{\\partial Z_3}(\\frac{\\partial Z_3}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_1} + 0 ) +\\frac{\\partial Z_5}{\\partial Z_4}(\\frac{\\partial Z_4}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_1} + 0) ) \\quad \\text{Since, } \\frac{\\partial Z_2}{\\partial W_1}=0 \\\\\n",
    "&= \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial Z_5}\\frac{\\partial Z_5}{\\partial Z_3}\\frac{\\partial Z_3}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_1} + \\frac{\\partial \\mathcal{J}(\\theta)}{\\partial Z_5}\\frac{\\partial Z_5}{\\partial Z_4}\\frac{\\partial Z_4}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_1}\n",
    "\\end{aligned}\n",
    "\n",
    "### Appendix D: Jacobian and problem with their size:\n",
    "Jacobian is a matrix which contains first order partial derivatives of 2 vectors(even multidimensional vectors). Let's say you have 2 vectors $x,y \\subset \\mathbb{R}^2$, then the jacobian is $\\frac{\\partial y}{\\partial x} \\subset \\mathbb{R}^{2*2}$, specifically:\n",
    "\\begin{equation} \\frac{\\partial y}{\\partial x} = \\begin{bmatrix} \\frac{\\partial y_0}{\\partial x_0} & \\frac{\\partial y_0}{\\partial x_1}\\\\ \\frac{\\partial y_1}{\\partial x_0} & \\frac{\\partial y_1}{\\partial x_1}\\end{bmatrix}\\end{equation}\n",
    "It tell how each element of x affects an element of y.\n",
    "To demonstrate how Jacobians tend to be huge, let's take the network we defined in first gif. \n",
    "$I \\subset \\mathbb{R}^{10*10}, W_1 \\subset \\mathbb{R}^{10*5}, Z_1 \\subset \\mathbb{R}^{10*5}, Z_3 \\subset \\mathbb{R}^{10*1},Z_4 \\subset \\mathbb{R}^{10*1},Z_5 \\subset \\mathbb{R}^{10*1}$\n",
    "So the size of different Jacobians we will use to compute gradients of $W_1$ are\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial Z_5} &= (1*10*1) &&= 10\\\\\n",
    "\\frac{\\partial Z_5}{\\partial Z_3} &= (10*1*10*1) &&= 100 \\\\\n",
    "\\frac{\\partial Z_3}{\\partial Z_1} &= (10*1*10*5) &&= 500\\\\\n",
    "\\frac{\\partial Z_1}{\\partial W_1} &= (10*5*10*5) &&= 2500\\\\\n",
    "\\frac{\\partial Z_5}{\\partial Z_4} &= (10*1*10*1) &&= 100\\\\\n",
    "\\frac{\\partial Z_4}{\\partial Z_1} &= (10*1*10*5) &&= 500\\\\\n",
    "& \\text{Total} &&= 3710\n",
    "\\end{aligned}\n",
    "We will have to find 3710 scalar values in order to find gradients of 10*5= 50 scalar values. So, you can see that finding gradients using jacobians is not a feasible approach when we increase the number of layers of increase the dimensions of our parameters.\n",
    "\n",
    "### Appendix E: Gradients in Autodiff\n",
    "![Gradients in Autodif](https://raw.githubusercontent.com/akashe/gifsandvids/main/Gradients%20in%20autodiff.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
